# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fy1LXU3u4cmdVH8-ILDdwdOPCbGCGrG-
"""

#Employee salary prediction using audlt csv
#Load all Libarary
import pandas as pd

data=pd.read_csv("/content/adult 3.csv")

"""data"""

data

data.shape

data.head(7)

data.tail()

#Null Values
data.isna()

data.isna().sum()

print(data.occupation.value_counts())

print(data.gender.value_counts())

print(data['marital-status'].value_counts())

print(data['education'].value_counts())

print(data['workclass'].value_counts())

data.occupation.replace({'?':'others'},inplace=True)

print(data.occupation.value_counts())

data

data.workclass.replace({'?':'NotListed'},inplace=True)

print(data.workclass.value_counts())

data

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data.workclass.value_counts())



data.shape

data=data[data['education']!='5th-6th']
data=data[data['education']!='1st-4th']
data=data[data['education']!='preschool']

print(data.education.value_counts())

data.shape

#redundancy
data.drop(columns=['education'],inplace=True)

data

#outlier
import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()

#outlier
import matplotlib.pyplot as plt
plt.boxplot(data['educational-num'])
plt.show()

data=data[(data['age']<=75) & (data['age']<=17)]

plt.boxplot(data['age'])
plt.show()

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
data['workclass']=encoder.fit_transform(data['workclass'])
data['marital-status']=encoder.fit_transform(data['marital-status'])
data['occupation']=encoder.fit_transform(data['occupation'])
data['race']=encoder.fit_transform(data['race'])
data['relationship']=encoder.fit_transform(data['relationship'])
data['native-country']=encoder.fit_transform(data['native-country'])
data

x=data.drop(columns=['income']) #input
y=data['income']   #output
x

y

from sklearn.preprocessing import MinMaxScaler, LabelEncoder

# First encode categorical string columns to numeric values
categorical_cols = ['workclass', 'marital-status', 'occupation',
                   'relationship', 'race', 'gender', 'native-country']

# Create a copy to avoid modifying original data
x_encoded = x.copy()

# Apply Label Encoding to each categorical column
for col in categorical_cols:
    if col in x_encoded.columns:
        le = LabelEncoder()
        x_encoded[col] = le.fit_transform(x_encoded[col])

# Now apply MinMax scaling
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(x_encoded)

# This will give you the scaled array output you want
x_scaled

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest=train_test_split(x,y,test_size=0.2, random_state=23,stratify=y)

xtrain= np.array(xtrain)

xtrain

from os import pipe
from sklearn.pipeline import Pipeline

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score, classification_report

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

from sklearn.neighbors import KNeighborsClassifier

from sklearn.svm import SVC

from sklearn.preprocessing import StandardScaler, OneHotEncoder

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {

"LogisticRegression": LogisticRegression(),

"RandomForest": RandomForestClassifier(),

"KNN": KNeighborsClassifier(),

"SVM": SVC(),

"GradientBoosting": GradientBoostingClassifier()
}
results={}

for name, model in models.items():

 pipe = Pipeline([

('scaler', StandardScaler()),

('model', model)

])

pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)

acc= accuracy_score(y_test, y_pred)

results [name] = acc

print(f"(name) Accuracy: (acc:.4f)")

print(classification_report(y_test, y_pred))

from google.colab import files
from io import BytesIO
import pandas as pd

# This will prompt you to upload a file
uploaded = files.upload()

# Get the filename of your uploaded file
file_name = list(uploaded.keys())[0]

# Load your data
data = pd.read_csv(BytesIO(uploaded[file_name]))
print("File successfully loaded!")
print("Columns in dataset:", data.columns.tolist())
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# 1. Check for missing classes in income column
print("\nIncome value counts:")
print(data['income'].value_counts())

# Clean income column
data['income'] = data['income'].str.strip()

# 2. Verify we have both classes
if len(data['income'].unique()) < 2:
    print("\nWarning: Only one class found in income column.")
    print("Showing first 5 rows of data:")
    print(data.head())
    raise ValueError("Dataset only contains one class. Cannot perform classification.")

# 3. Prepare features and target
x = data.drop(columns=['income'])
y = data['income']

# 4. Split data (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    x, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 5. Set up preprocessing
numeric_cols = x.select_dtypes(include=['number']).columns
categorical_cols = x.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# 6. Create and train pipeline
pipe = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

pipe.fit(X_train, y_train)

# 7. Evaluate
y_pred = pipe.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
from sklearn.ensemble import IsolationForest

print("\nRunning anomaly detection instead...")

# Preprocess features
X = preprocessor.fit_transform(x)

# Train anomaly detector
model = IsolationForest(contamination=0.05, random_state=42)
model.fit(X)

# Get predictions (-1 = anomaly, 1 = normal)
anomalies = model.predict(X)
data['anomaly_score'] = model.decision_function(X)
data['is_anomaly'] = anomalies

print("\nAnomaly detection results:")
print(data['is_anomaly'].value_counts())
print("\nTop 5 potential anomalies:")
print(data[data['is_anomaly'] == -1].head())

import matplotlib.pyplot as plt

plt.bar(results.keys(), results.values(), color='skyblue')

plt.ylabel('Accuracy Score')

plt.title('Model Comparison')

plt.xticks(rotation=45)

plt.grid(True)

plt.show()

from google.colab import files
from io import BytesIO
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import joblib

# 1. Upload and load data
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
data = pd.read_csv(BytesIO(uploaded[file_name]))
print("File successfully loaded!")
print("Columns in dataset:", data.columns.tolist())

# 2. Data preparation
print("\nIncome value counts:")
print(data['income'].value_counts())

# Clean income column
data['income'] = data['income'].str.strip()

# Verify we have both classes
if len(data['income'].unique()) < 2:
    print("\nWarning: Only one class found in income column.")
    print("Showing first 5 rows of data:")
    print(data.head())
    raise ValueError("Dataset only contains one class. Cannot perform classification.")

# 3. Prepare features and target
x = data.drop(columns=['income'])
y = data['income']

# 4. Split data (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    x, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 5. Set up preprocessing
numeric_cols = x.select_dtypes(include=['number']).columns
categorical_cols = x.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# 6. Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

# 7. Train and evaluate models
results = {}
for name, model in models.items():
    try:
        pipe = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)
        ])

        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_test)
        acc = accuracy_score(y_test, preds)
        results[name] = acc

        print(f"\n{name}: {acc:.4f}")
        print(classification_report(y_test, preds))

    except Exception as e:
        print(f"\nError with {name}: {str(e)}")

# 8. Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\nBest model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# 9. Save the best model
joblib.dump(best_model, "best_model.pkl")
print("Saved best model as best_model.pkl")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# from sklearn.ensemble import GradientBoostingClassifier
# 
# # Load or create model
# try:
#     model = joblib.load("best_model.pkl")
#     if not isinstance(model, GradientBoostingClassifier):
#         model = GradientBoostingClassifier()
# except:
#     model = GradientBoostingClassifier()
# 
# # App setup
# st.set_page_config(page_title="Salary Prediction", layout="centered")
# st.title("💰 Employee Salary Prediction")
# st.markdown("Predict salary class (>50K or ≤50K) using Gradient Boosting")
# 
# # Sidebar inputs
# st.sidebar.header("Employee Details")
# age = st.sidebar.slider("Age", 18, 65, 22)
# education = st.sidebar.selectbox("Education", ["Bachelors", "Masters", "HS-grad", "Some-college"])
# occupation = st.sidebar.selectbox("Occupation", ["Tech-support", "Sales", "Exec-managerial"])
# hours = st.sidebar.slider("Hours/week", 1, 80, 40)
# experience = st.sidebar.slider("Experience (years)", 0, 40, 5)
# 
# # Display input
# st.header("Input Data")
# input_df = pd.DataFrame({
#     'age': [age],
#     'education': [education],
#     'occupation': [occupation],
#     'hours-per-week': [hours],
#     'experience': [experience]
# })
# st.table(input_df)
# 
# # Prediction
# if st.button("Predict Salary Class"):
#     try:
#         pred = model.predict(input_df)[0]
#         proba = model.predict_proba(input_df)[0][1] if hasattr(model, 'predict_proba') else 0.75
#         result = ">50K" if pred == 1 else "≤50K"
#         st.success(f"Prediction: {result} (Confidence: {proba:.0%})")
#     except Exception as e:
#         st.error(f"Error: {str(e)}")
# 
# # Batch prediction
# st.header("Batch Prediction")
# uploaded_file = st.file_uploader("Upload CSV", type="csv")
# if uploaded_file:
#     batch = pd.read_csv(uploaded_file)
#     st.write("First 3 rows:", batch.head(3))
# 
#     if st.button("Predict Batch"):
#         try:
#             predictions = model.predict(batch)
#             batch['Prediction'] = [">50K" if p == 1 else "≤50K" for p in predictions]
#             st.write("Results:", batch.head())
#         except Exception as e:
#             st.error(f"Batch error: {str(e)}")!pip install streamlit

!pip install streamlit pyngrok

!ngrok authtoken 30EWiuhlkZhNP1tHbyRmlvO24fs_3EkLYGsVmUjL8yYheGnHG

!pip install streamlit pyngrok

import os
import threading
import time
from pyngrok import ngrok

# Corrected function to run Streamlit
def run_streamlit():
    os.system("streamlit run app.py --server.port 8501")

# Create and start the thread
thread = threading.Thread(target=run_streamlit)
thread.start()

from pyngrok import ngrok

import time

#Wait a few seconds to make sure Streamlit started

time.sleep(5)

#Create a tunnel to the Streamlit port 8501

public_url = ngrok.connect(8501)

print("Your Streamlit app is live here:", public_url)
